{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502418dd-96a6-485d-b441-757f272b5b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning Rate Scheduler for Optimizers\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html#module-torch.optim.lr_scheduler\n",
    "\n",
    "### What is lr_scheduler?\n",
    "\n",
    "    torch.optim.lr_scheduler\n",
    "    \n",
    "> The lr_scheduler module in PyTorch provides a variety of strategies to adjust the learning rate dynamically during training.\n",
    "\n",
    "\n",
    "### Common Learning Rate Schedulers in PyTorch\n",
    ">- LambdaLR\n",
    ">- StepLR\n",
    ">- MultiStepLR\n",
    ">- ExponentialLR\n",
    ">- ReduceLROnPlateau\n",
    ">- CosineAnnealingLR\n",
    "\n",
    "## How to Use lr_scheduler\n",
    ">  Learning rate scheduling should be applied after optimizer’s update\n",
    ">\n",
    "    1- Define Model and Optimizer\n",
    "    2- Initialize the Scheduler\n",
    "    3- Train :\n",
    "    \n",
    "        for epoch in range(train_loader):\n",
    "             for batch in range(all_batches):\n",
    "                 optimizer.zero_grad()\n",
    "                 predictions = model(x)\n",
    "                 loss = MSELoss(label,predictions)\n",
    "                 loss.backward()\n",
    "                 optimizer.step()\n",
    "             scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d055f-08e9-41e1-af25-05c97ad72c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.optim import lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31332f51-41fe-4c30-b2ea-7b1564a9049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine device for computations (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Mean and standard deviation for normalization\n",
    "mean = 0.13066048920154572\n",
    "std  = 0.30810779333114624\n",
    "\n",
    "# Batch size for DataLoader\n",
    "batch_size = 128\n",
    "\n",
    "# Transformation pipeline for training data\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(5),               # Random rotation up to 5 degrees\n",
    "    transforms.RandomCrop(28, padding=2),      # Random crop with padding of 2 pixels\n",
    "    transforms.ToTensor(),                     # Convert to tensor\n",
    "    transforms.Normalize(mean=[mean], std=[std])   # Normalize with predefined mean and std\n",
    "])\n",
    "\n",
    "# Load training dataset\n",
    "train_data = datasets.MNIST(root='.data', \n",
    "                            train=True, \n",
    "                            download=True, \n",
    "                            transform=train_transforms)\n",
    "\n",
    "# DataLoader for training data\n",
    "train_loader = data.DataLoader(train_data, \n",
    "                               shuffle=True,    # Shuffle the data\n",
    "                               batch_size=batch_size)  # Batch size for training\n",
    "\n",
    "# Transformation pipeline for testing data\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                     # Convert to tensor\n",
    "    transforms.Normalize(mean=[mean], std=[std])   # Normalize with predefined mean and std\n",
    "])\n",
    "\n",
    "# Load testing dataset\n",
    "test_data = datasets.MNIST(root='.data', \n",
    "                           train=False, \n",
    "                           download=False, \n",
    "                           transform=test_transforms)\n",
    "\n",
    "# DataLoader for testing data\n",
    "test_loader = data.DataLoader(test_data,  \n",
    "                              batch_size=batch_size)  # Batch size for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce3f9d-d74c-4ccd-b046-6ecc9013ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # --\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        # ----\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9bd96-efb3-475d-be57-d0135c51da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def train_epoch(iterator, model, optimizer, criterion, device):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model.train()\n",
    "    lrs = []\n",
    "    epoch_loss = []\n",
    "    for images, labels in tqdm.tqdm(iterator, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    return epoch_loss  , lrs\n",
    "\n",
    "def train(train_iterator, model, optimizer, scheduler, criterion, device, n_epochs=5):\n",
    "    \"\"\"Trains the model for the given number of epochs.\"\"\"\n",
    "    \n",
    "    lrs = []\n",
    "    train_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "       \n",
    "        epoch_loss, epoch_lrs = train_epoch(train_iterator, model, optimizer, criterion, device)\n",
    "         \n",
    "        train_losses.extend(epoch_loss)\n",
    "        lrs.extend(epoch_lrs)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}\", 'lr: ',scheduler.get_last_lr() )\n",
    "        scheduler.step()  # Update learning rate after each epoch\n",
    "        \n",
    "    \n",
    "    return train_losses ,lrs \n",
    "\n",
    "def plot_graphs(value_list, labels, title=None, ymin=0, ymax=None, figsize=(15,5)):\n",
    "    \"\"\"Plots the losses from multiple experiments.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for value, label in zip(value_list, labels):\n",
    "        ax.plot(value, label=label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Update Steps')\n",
    "    ax.set_ylim(ymin=ymin, ymax=ymax)\n",
    "    ax.grid()\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    # Set the model to evaluation mode to turn off dropout, batch normalization, etc.\n",
    "    model.eval()\n",
    " \n",
    "    # Initialize counters for overall accuracy\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    # Initialize counters for class-wise accuracy\n",
    "    n_class_correct = [0 for _ in range(10)]\n",
    "    n_class_samples = [0 for _ in range(10)]\n",
    "    \n",
    "    # Define the class names for easier readability\n",
    "    classes = ('0' ,'1', '2', '3', '4', '5', '6', '7', '8', '9' )\n",
    "    \n",
    "    # Disable gradient calculation since we are in inference mode\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test data loader\n",
    "        for images, labels in test_loader:\n",
    "            # Move the images and labels to the specified device (CPU or GPU)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Perform the forward pass to get model predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get the predicted class by taking the index with the highest score\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Update the overall sample and correct prediction counters\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update the class-wise correct prediction counters\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if label == pred:\n",
    "                    n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "    \n",
    "    # Calculate and print the overall accuracy\n",
    "    overall_acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Overall accuracy: {overall_acc:.2f} %')\n",
    "    print('-'*25)\n",
    "    # Calculate and print the accuracy for each class\n",
    "    for i in range(10):\n",
    "        if n_class_samples[i] > 0:\n",
    "            class_acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "            print(f'Accuracy of {classes[i]}: {class_acc:.2f} %')\n",
    "        else:\n",
    "            print(f'Accuracy of {classes[i]}: No samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873de4a-a363-490f-ac3e-e78eaf68c4e7",
   "metadata": {},
   "source": [
    "## **torch.optim.lr_scheduler**\n",
    "\n",
    "    provides several methods to adjust the learning rate based on the number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88b7db-e831-4b92-9c22-c46bcdf0d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d8a2c-e732-4f52-8cab-a379b1c77413",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d45f9-087e-4534-8a6b-d91807092965",
   "metadata": {},
   "source": [
    "    torch.optim.SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b1f556-578e-4c18-99c9-445d151b6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f2e75-900e-4f84-bd85-ac2d3d157eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71463f-3e12-4bdd-852c-421c74255f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['momentum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fb075-19ea-473e-a52d-b87c628c4e7a",
   "metadata": {},
   "source": [
    "# 1. LambdaLR\n",
    "\n",
    "    Uses a user-defined lambda function to adjust the learning rate.\n",
    "\n",
    "\n",
    "#### Parameters\n",
    "\n",
    ">- optimizer (Optimizer) – Wrapped optimizer.\n",
    ">- lr_lambda (function or list) – A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups.\n",
    ">- last_epoch (int) – The index of last epoch. Default: -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfa7e2-badc-455e-b4ee-7f71a1e609d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambda_fn = lambda epoch: 0.65 ** epoch\n",
    "# Example: learning rate decays by a factor of 0.65 every epoch\n",
    "# Epoch 1: Learning Rate: 0.1\n",
    "# Epoch 2: Learning Rate: 0.065\n",
    "# Epoch 3: Learning Rate: 0.04225\n",
    "# Epoch 4: Learning Rate: 0.0274625\n",
    "# Epoch 5: Learning Rate: 0.01787125\n",
    "\n",
    "LambdaLR_scheduler =  lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_fn)\n",
    "LambdaLR_loss, LambdaLR_lrs  = train(train_loader, model=model, optimizer=optimizer,scheduler=LambdaLR_scheduler, criterion=criterion, device=device, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c8235-2892-4640-a068-253ae2dbe395",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( LambdaLR_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca3cfe-8a5e-4293-a9e5-d01d6c9bbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( LambdaLR_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca8ba2-367e-4ad9-825b-e0aba2508c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs( [LambdaLR_lrs,LambdaLR_loss],['LambdaLR_lrs','LambdaLR_loss'],ymax=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37860b6f-4131-4e67-8f6d-ea0bf6317c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970241e0-0913-4db4-8581-7544b1d190d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LambdaLR_scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d91dff-7d4d-4fe6-b45a-0ed28c5720f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. StepLR\n",
    "\n",
    "Decreases the learning rate by a factor every few epochs.\n",
    "\n",
    "    torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "\n",
    "#### Parameters\n",
    "\n",
    ">- optimizer (Optimizer) – Wrapped optimizer.\n",
    ">- step_size (int) – Period of learning rate decay.\n",
    ">- gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.\n",
    ">- last_epoch (int) – The index of last epoch. Default: -1.\n",
    "\n",
    "\n",
    "#### step_size:\n",
    ">It defines the number of epochs after which the learning rate will be reduced. In this case, it's set to 7 epochs.\n",
    "\n",
    "#### gamma: \n",
    ">This is the factor by which the learning rate will be multiplied. Here, the learning rate will be multiplied by 0.1 every 7 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8fac3-af13-4bc4-ad40-de5db50881c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "steplr_scheduler =  lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "steplr_loss,steplr_lrs = train(train_loader, model=model, optimizer=optimizer,scheduler=steplr_scheduler, criterion=criterion, device=device, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee16dd-c6be-44f4-a898-b34f7f26ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steplr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc52537",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steplr_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a4337-51c5-4349-a69a-535824903704",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs( [steplr_lrs,steplr_loss],['steplr_lrs','steplr_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c04e9-a406-4e57-97c1-a2c82f1a90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_model(model, test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcebb94-9733-4c5e-abfb-356a30c10caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "steplr_scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd3188-747b-42b4-8557-94bd880d6037",
   "metadata": {},
   "source": [
    "# 3. MultiStepLR\n",
    "\n",
    "Decreases the learning rate at specific epochs.\n",
    "\n",
    "\n",
    "    torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "#### Parameters\n",
    "> - optimizer (Optimizer) – Wrapped optimizer.\n",
    "> - milestones (list) – List of epoch indices. Must be increasing.\n",
    "> - gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.\n",
    "> - last_epoch (int) – The index of last epoch. Default: -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc60bda-905e-4372-b362-7fec4219a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "MultiStepLR_scheduler =  lr_scheduler.MultiStepLR(optimizer, milestones=[2,3,4], gamma=0.1)\n",
    "MultiStepLR_scheduler_loss,MultiStepLR_scheduler_lrs = train(train_loader, model=model, optimizer=optimizer,scheduler=MultiStepLR_scheduler, criterion=criterion, device=device, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b87de-7e41-4c6d-97a4-6e224156a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MultiStepLR_scheduler_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aff46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MultiStepLR_scheduler_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b57346-526b-4390-ae92-fcd0f6b9de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs( [MultiStepLR_scheduler_lrs,MultiStepLR_scheduler_loss],\n",
    "            ['MultiStepLR_scheduler_lrs','MultiStepLR_scheduler_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9969f-8f47-49e7-b43e-e7cd8cba068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_model(model, test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964c3c9-e5c5-4dcc-be5a-288fe9ecbf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiStepLR_scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de9e56-261d-43af-959c-9a35ce7d5951",
   "metadata": {},
   "source": [
    "# 4. ExponentialLR\n",
    "\n",
    "Decays the learning rate exponentially.\n",
    "\n",
    "    torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)\n",
    "    \n",
    "#### Parameters\n",
    ">- optimizer (Optimizer) – Wrapped optimizer.\n",
    ">- gamma (float) – Multiplicative factor of learning rate decay.\n",
    ">- last_epoch (int) – The index of last epoch. Default: -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc0aca-fa8b-4d81-a3b2-64c714afbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "learning_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "ExponentialLR_scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "ExponentialLR_loss,ExponentialLR_lrs= train(train_loader, model=model, optimizer=optimizer,scheduler=ExponentialLR_scheduler, criterion=criterion, device=device, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fe0e5-b8d0-4f2b-aafb-13b9b049d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ExponentialLR_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ExponentialLR_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fa65d-1f4c-4e4f-b953-1fde35424d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs( [ExponentialLR_lrs,ExponentialLR_loss],['ExponentialLR_lrs','ExponentialLR_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c960e-10bb-4bf3-923b-385f388a8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_model(model, test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1559d-0036-4d6a-9810-2879776c0af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExponentialLR_scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9154e-9570-4c61-9f77-db291eb2f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs([ LambdaLR_loss , steplr_loss , MultiStepLR_scheduler_loss , ExponentialLR_loss,], \n",
    "            ['LambdaLR_loss','steplr_loss','MultiStepLR_scheduler_loss','ExponentialLR_loss'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d4f38-e613-474f-af3f-291b40fbb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs([LambdaLR_lrs , steplr_lrs , MultiStepLR_scheduler_lrs , ExponentialLR_lrs],\n",
    "           ['LambdaLR_lrs','steplr_lrs','MultiStepLR_scheduler_lrs','ExponentialLR_lrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e614d-6924-4f96-83bd-31fd089d3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next epoch\n",
    "print(LambdaLR_scheduler.get_last_lr() , steplr_scheduler.get_last_lr() , MultiStepLR_scheduler.get_last_lr(), ExponentialLR_scheduler.get_last_lr())      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50caa7f2-15dd-4194-a51a-faa57cc83a55",
   "metadata": {},
   "source": [
    " # Using ConvNet for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ab6f5-d62e-4f07-85fe-63d8e5004ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Hyperparameters\n",
    " \n",
    "batch_size = 128\n",
    " \n",
    "# Data transforms\n",
    "transform_train2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "\n",
    "transform_test2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "\n",
    "# Data  \n",
    "train_dataset2 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train2)\n",
    "test_dataset2 = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test2)\n",
    "# Data loader\n",
    "train_loader2 = torch.utils.data.DataLoader(train_dataset2,batch_size = batch_size, shuffle =True)\n",
    "test_loader2  = torch.utils.data.DataLoader(test_dataset2,batch_size  = batch_size, shuffle =False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30de42c-a0be-499f-a40f-aad3ce03207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the  CNN with Batch Norm\n",
    "class CNN_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32) \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64) \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    " #              ((INPUT SIZE - FILTER SIZE + 2 PADDING) / STRIDE  ) + 1\n",
    "        #image -> 32 x 32\n",
    "        #conv1 ->      32    -      3      +   2      ) /     1   ) + 1 =  32\n",
    "        #pool  ->      32    -      2      +   0      ) /     2   ) + 1 =  16\n",
    "        #conv2 ->      16    -      3      +   2      ) /     1   ) + 1 =  16\n",
    "        #pool  ->      16    -      2      +   0      ) /     2   ) + 1 =  8 \n",
    "        #conv3 ->       8    -      3      +   2      ) /     1   ) + 1 =  8\n",
    "        #pool  ->       8    -      2      +   0      ) /     2   ) + 1 =  4\n",
    "        #      -> 4 x 4  \n",
    "        \n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767964a-7a93-431e-87e2-744693011bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epochs = 25\n",
    "cnn_model = CNN_BN().to(device)\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be46c62-7704-43c6-8515-2526d51c2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch   1         5         10           15          20          25\n",
    "# gamma  0.5       0.5       0.5          0.5          0.5\n",
    "# lr     0.01 --> 0.005 --> 0.0025 --> 0.00125  --> 0.000625 --> 0.0003125\n",
    "\n",
    "steplr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "steplr_scheduler_loss,steplr_scheduler_lrs = train(train_loader2, model=cnn_model, optimizer=optimizer,scheduler=steplr_scheduler, criterion=criterion, device=device, n_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f57cec-8e37-4697-b4d6-90fdb5e5a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steplr_scheduler_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1400b3-63af-41b0-a2b7-bc3ca192187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steplr_scheduler_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f9907-37ca-4c2e-ae7a-1332dea9d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs([steplr_scheduler_lrs , steplr_scheduler_loss ], \n",
    "           ['steplr_scheduler_lrs','steplr_scheduler_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568cbab-e3f6-456a-8f4b-103d9aa8356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_model(cnn_model, test_loader2, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce87d0-589f-4dd5-98ea-b5bc8ffa5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6c302-03ab-4d77-a3ac-713892d1a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6a128-3362-431d-9601-e2b9a1091de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5392ed-1586-46d6-909d-973f39d264cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9c0dc-ca80-41da-9958-181f50e6b203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f78c6-281a-43e1-b718-f32a34a754d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
